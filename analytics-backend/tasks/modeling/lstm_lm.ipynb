{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LSTM_CONFIG = {\n",
    "    'vocab_size': 32008,\n",
    "    'embedding_size': 128,\n",
    "    'embedding_factor_size': 300,\n",
    "    'hidden_size': 1024,\n",
    "    'n_layers': 3\n",
    "}\n",
    "\n",
    "DEFAULT_DISCRIMINATOR_CONFIG = {\n",
    "    'encoder_hidden_size': 1024,\n",
    "    'hidden_size': 512\n",
    "}\n",
    "\n",
    "DEFAULT_GENERATOR_CONFIG = {\n",
    "    'encoder_hidden_size': 768,\n",
    "    'hidden_size': 256,\n",
    "    'max_sequence_length': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM(nn.Module):\n",
    "\n",
    "    def __init__(self, config={}):\n",
    "        self.config = DEFAULT_LSTM_CONFIG\n",
    "        self.config.update(config)\n",
    "        \n",
    "        self.vocab_size = self.config['vocab_size']\n",
    "        self.embedding_size = self.config['embedding_size']\n",
    "        self.embedding_factor_size = self.config['embedding_factor_size']\n",
    "        self.hidden_size = self.config['hidden_size']\n",
    "        self.n_layers = self.config['n_layers']\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.embedding_linear = nn.Linear(self.embedding_size, self.embedding_factor_size)\n",
    "        self.rnn = nn.LSTM(self.embedding_factor_size, self.hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True, \n",
    "            n_layers=self.n_layers\n",
    "        )\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        x = self.embedding(tokens)\n",
    "        x = self.embedding_linear(tokens)\n",
    "\n",
    "        x[x == 0] = -1e9\n",
    "        x = torch.max(x, 0)[0]\n",
    "        if x.ndimension() == 3:\n",
    "            x = x.squeeze(0)\n",
    "            assert x.ndimension() == 2\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDiscriminatorHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config={}):\n",
    "        \n",
    "        self.config = DEFAULT_DISCRIMINATOR_CONFIG\n",
    "        self.config.update(config)\n",
    "\n",
    "        self.encoder_hidden_size = self.config['encoder_hidden_size']\n",
    "        self.hidden_size = self.config['hidden_size']\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder_hidden_size * 2, self.hidden_size),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Linear(self.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMGeneratorHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config={}):\n",
    "        \n",
    "        self.config = DEFAULT_GENERATOR_CONFIG\n",
    "        self.config.update(config)\n",
    "\n",
    "        self.encoder_hidden_size = self.config['encoder_hidden_size']\n",
    "        self.hidden_size = self.config['hidden_size']\n",
    "        self.max_sequence_length = self.config['max_sequence_length']\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder_hidden_size * 2, self.hidden_size),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Linear(self.hidden_size, self.max_sequence_length)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}